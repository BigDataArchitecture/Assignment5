{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b66c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17226ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.loads(event['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf43aca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).',\n",
       " 'question': 'What is the GLUE score for Bert?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240833e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424d7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"body\":\"{\\\"context\\\":\\\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\",\\n\\\"question\\\":\\\"What is the GLUE score for Bert?\\\"\\n}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96428d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adba627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"context\":\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\",\\n\"question\":\"What is the GLUE score for Bert?\"\\n}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335f795d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "event['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b968bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoConfig\n",
    "\n",
    "def encode(tokenizer, question, context):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(question, context)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "def decode(tokenizer, token):\n",
    "    \"\"\"decodes the tokens to the answer with a given tokenizer\"\"\"\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        token, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "def serverless_pipeline(model_path='./model'):\n",
    "    \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    def predict(question, context):\n",
    "        \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\"\n",
    "        input_ids, attention_mask = encode(tokenizer,question, context)\n",
    "        a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "        start_scores = a['start_logits']\n",
    "        end_scores = a['end_logits']\n",
    "        ans_tokens = input_ids[torch.argmax(\n",
    "            start_scores): torch.argmax(end_scores)+1]\n",
    "        answer = decode(tokenizer,ans_tokens)\n",
    "        return answer\n",
    "    return predict\n",
    "\n",
    "# initializes the pipeline\n",
    "question_answering_pipeline = serverless_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2e07325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80. 5 %'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path='./model'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "input_ids, attention_mask = encode(tokenizer,body['question'], body['context'])\n",
    "a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "start_scores = a['start_logits']\n",
    "end_scores = a['end_logits']\n",
    "ans_tokens = input_ids[torch.argmax(\n",
    "            start_scores): torch.argmax(end_scores)+1]\n",
    "answer = decode(tokenizer,ans_tokens)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5cae9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(180)\n",
      "tensor(184)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(start_scores))\n",
    "print(torch.argmax(end_scores)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc8a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = question_answering_pipeline(question=body['question'], context=body['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3a83513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.9935,  -9.6032, -12.6266,  -8.8786,  -9.7391, -10.4422, -13.1308,\n",
       "         -10.5575, -13.7492, -11.6378,  -8.5799, -10.5312,  -9.8652,  -9.7580,\n",
       "          -9.4949, -11.9562, -12.3860, -12.0936,  -8.6679, -12.0692, -10.2790,\n",
       "         -10.8464, -12.4050,  -7.5609, -12.8723, -12.9467, -13.6367,  -9.9548,\n",
       "         -12.5127, -13.8898, -10.5344, -12.8058,  -9.0189, -11.6492, -10.1382,\n",
       "          -9.3935,  -9.7570, -12.3048, -12.3742, -10.0070,  -7.4577, -11.7353,\n",
       "         -13.9000, -14.5311, -12.7638,  -8.2836, -13.4187, -11.9397,  -7.5125,\n",
       "         -12.0522, -11.9577, -13.0190, -13.8692, -13.9177, -12.6028,  -8.6266,\n",
       "         -12.6308, -11.2395,  -8.2875, -11.5319,  -9.7516,  -9.8484,  -8.8828,\n",
       "         -12.1323,  -9.1497,  -9.5073, -14.3122, -14.7056, -15.2779, -12.3574,\n",
       "         -12.2021,  -8.8719, -12.2928, -13.3568, -13.6476, -10.2934,  -9.7695,\n",
       "          -8.4249,  -9.5299, -10.4064,  -8.4501,  -8.1455, -13.3144,  -9.5901,\n",
       "          -9.9255,  -9.9359,  -8.9109, -10.1722, -10.9245, -10.2094, -11.4849,\n",
       "         -12.8439, -11.3473,  -7.2822,  -7.7438, -12.2276, -10.2857,  -9.2049,\n",
       "         -11.3570, -10.1022, -12.0572,  -8.6917, -12.5233, -13.2024, -10.5926,\n",
       "          -8.1687,  -6.5877,  -9.6355,  -9.4750, -10.5913, -10.6655, -10.7288,\n",
       "          -9.7544, -14.1949, -12.8981, -13.8851, -11.2254, -14.0085, -12.2946,\n",
       "         -11.9040, -12.2679,  -9.8179,  -9.8613, -12.0148, -15.1357, -12.1575,\n",
       "         -11.7155, -11.8134, -13.9296,  -9.4054, -12.0592, -13.9847,  -9.5476,\n",
       "         -12.0704, -13.0676,  -9.9968,  -8.8959,  -9.2934, -11.9428, -13.6875,\n",
       "         -14.3590, -14.0119, -10.9463, -10.7517, -10.8265,  -7.8211, -11.1233,\n",
       "          -8.5499, -13.8656,  -8.8626, -13.8940,  -9.5735, -14.5186, -10.1458,\n",
       "         -10.6286,  -6.7452,  -9.0446, -13.2036,  -8.1718,  -8.4822, -13.1104,\n",
       "         -11.9360, -12.8214, -10.6498, -13.0211, -10.9861,  -9.8080,  -8.9694,\n",
       "          -6.9046,  -8.5696,  -8.3411, -10.3213, -10.1375,  -9.6618,  -7.4663,\n",
       "          -4.4304,  -4.1391,  -5.1907,  -7.4831,  -3.0677,   2.3072,  -7.7782,\n",
       "          -6.5240,  -4.7309,  -4.8970,  -0.3418,  -9.9202,  -7.7213,  -8.0191,\n",
       "          -8.0278,  -5.9619,  -8.0912,  -8.5301,  -9.8071,  -4.8425,  -9.9268,\n",
       "         -10.6415,  -7.5797,  -7.0113,  -2.6098, -11.2151,  -9.6228,  -8.0463,\n",
       "          -7.6116,  -4.1799, -11.3936,  -9.0563,  -9.1889,  -8.6666, -10.1769,\n",
       "         -10.1953, -11.2783,  -6.2193,  -8.7358,  -9.7729, -12.4952,  -9.9106,\n",
       "          -8.8787, -10.7488, -10.8436,  -8.5129,  -9.9556,  -5.1896, -12.7473,\n",
       "          -9.7157, -10.8408,  -5.6944, -11.8643,  -9.6395, -12.0872, -10.6213,\n",
       "         -12.0420, -12.0264, -11.5038,  -6.4843,  -8.9994, -10.1185, -13.1069,\n",
       "         -10.8206, -10.1366,  -9.0342,  -9.5118,  -4.5315, -12.5122,  -9.6609,\n",
       "         -10.1086,  -5.9065, -12.2503,  -9.9088, -11.9210, -10.8050, -11.9623,\n",
       "         -11.8584,  -9.5306,  -9.3333]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "724ba745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2054,\n",
       " 2003,\n",
       " 1996,\n",
       " 25238,\n",
       " 3556,\n",
       " 2005,\n",
       " 14324,\n",
       " 1029,\n",
       " 102,\n",
       " 2057,\n",
       " 8970,\n",
       " 1037,\n",
       " 2047,\n",
       " 2653,\n",
       " 6630,\n",
       " 2944,\n",
       " 2170,\n",
       " 14324,\n",
       " 1010,\n",
       " 2029,\n",
       " 4832,\n",
       " 2005,\n",
       " 7226,\n",
       " 7442,\n",
       " 7542,\n",
       " 2389,\n",
       " 4372,\n",
       " 16044,\n",
       " 2099,\n",
       " 15066,\n",
       " 2013,\n",
       " 19081,\n",
       " 1012,\n",
       " 4406,\n",
       " 3522,\n",
       " 2653,\n",
       " 6630,\n",
       " 4275,\n",
       " 1006,\n",
       " 12420,\n",
       " 3802,\n",
       " 2632,\n",
       " 1012,\n",
       " 1010,\n",
       " 2760,\n",
       " 2050,\n",
       " 1025,\n",
       " 10958,\n",
       " 20952,\n",
       " 8551,\n",
       " 3802,\n",
       " 2632,\n",
       " 1012,\n",
       " 1010,\n",
       " 2760,\n",
       " 1007,\n",
       " 1010,\n",
       " 14324,\n",
       " 2003,\n",
       " 2881,\n",
       " 2000,\n",
       " 3653,\n",
       " 23654,\n",
       " 2784,\n",
       " 7226,\n",
       " 7442,\n",
       " 7542,\n",
       " 2389,\n",
       " 15066,\n",
       " 2013,\n",
       " 4895,\n",
       " 20470,\n",
       " 12260,\n",
       " 2094,\n",
       " 3793,\n",
       " 2011,\n",
       " 10776,\n",
       " 14372,\n",
       " 2006,\n",
       " 2119,\n",
       " 2187,\n",
       " 1998,\n",
       " 2157,\n",
       " 6123,\n",
       " 1999,\n",
       " 2035,\n",
       " 9014,\n",
       " 1012,\n",
       " 2004,\n",
       " 1037,\n",
       " 2765,\n",
       " 1010,\n",
       " 1996,\n",
       " 3653,\n",
       " 1011,\n",
       " 4738,\n",
       " 14324,\n",
       " 2944,\n",
       " 2064,\n",
       " 2022,\n",
       " 2986,\n",
       " 8525,\n",
       " 7228,\n",
       " 2007,\n",
       " 2074,\n",
       " 2028,\n",
       " 3176,\n",
       " 6434,\n",
       " 6741,\n",
       " 2000,\n",
       " 3443,\n",
       " 2110,\n",
       " 1011,\n",
       " 1997,\n",
       " 1011,\n",
       " 1996,\n",
       " 1011,\n",
       " 2396,\n",
       " 4275,\n",
       " 2005,\n",
       " 1037,\n",
       " 2898,\n",
       " 2846,\n",
       " 1997,\n",
       " 8518,\n",
       " 1010,\n",
       " 2107,\n",
       " 2004,\n",
       " 3160,\n",
       " 10739,\n",
       " 1998,\n",
       " 2653,\n",
       " 28937,\n",
       " 1010,\n",
       " 2302,\n",
       " 6937,\n",
       " 8518,\n",
       " 5051,\n",
       " 6895,\n",
       " 8873,\n",
       " 2278,\n",
       " 4294,\n",
       " 12719,\n",
       " 1012,\n",
       " 14324,\n",
       " 2003,\n",
       " 17158,\n",
       " 2135,\n",
       " 3722,\n",
       " 1998,\n",
       " 17537,\n",
       " 2135,\n",
       " 3928,\n",
       " 1012,\n",
       " 2009,\n",
       " 6855,\n",
       " 2015,\n",
       " 2047,\n",
       " 2110,\n",
       " 1011,\n",
       " 1997,\n",
       " 1011,\n",
       " 1996,\n",
       " 1011,\n",
       " 2396,\n",
       " 3463,\n",
       " 2006,\n",
       " 5408,\n",
       " 3019,\n",
       " 2653,\n",
       " 6364,\n",
       " 8518,\n",
       " 1010,\n",
       " 2164,\n",
       " 6183,\n",
       " 1996,\n",
       " 25238,\n",
       " 3556,\n",
       " 2000,\n",
       " 3770,\n",
       " 1012,\n",
       " 1019,\n",
       " 1003,\n",
       " 1006,\n",
       " 1021,\n",
       " 1012,\n",
       " 1021,\n",
       " 1003,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1010,\n",
       " 4800,\n",
       " 20554,\n",
       " 2072,\n",
       " 10640,\n",
       " 2000,\n",
       " 6564,\n",
       " 1012,\n",
       " 1021,\n",
       " 1003,\n",
       " 1006,\n",
       " 1018,\n",
       " 1012,\n",
       " 1020,\n",
       " 1003,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1010,\n",
       " 4686,\n",
       " 1058,\n",
       " 2487,\n",
       " 1012,\n",
       " 1015,\n",
       " 3160,\n",
       " 10739,\n",
       " 3231,\n",
       " 20069,\n",
       " 2000,\n",
       " 6109,\n",
       " 1012,\n",
       " 1016,\n",
       " 1006,\n",
       " 1015,\n",
       " 1012,\n",
       " 1019,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1998,\n",
       " 4686,\n",
       " 1058,\n",
       " 2475,\n",
       " 1012,\n",
       " 1014,\n",
       " 3231,\n",
       " 20069,\n",
       " 2000,\n",
       " 6640,\n",
       " 1012,\n",
       " 1015,\n",
       " 1006,\n",
       " 1019,\n",
       " 1012,\n",
       " 1015,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdeb5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb7b69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='hoanhkhoa/roberta-base-finetuned-ner', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6dfa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 20920, 6, 127, 2335, 16, 11962, 2] [1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def encode(tokenizer, question):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(question)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "def decode(tokenizer, token):\n",
    "    \"\"\"decodes the tokens to the answer with a given tokenizer\"\"\"\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        token, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "\n",
    "input_ids, attention_mask = encode(tokenizer,\"Hello, my dog is cute\")\n",
    "print(input_ids, attention_mask)\n",
    "a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "ans_tokens= input_ids[4]\n",
    "answer = decode(tokenizer,ans_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc168a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.9998253, 'index': 1, 'word': 'ĠMy', 'start': 0, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.9998684, 'index': 2, 'word': 'Ġname', 'start': 3, 'end': 7}, {'entity': 'LABEL_0', 'score': 0.99989426, 'index': 3, 'word': 'Ġis', 'start': 8, 'end': 10}, {'entity': 'LABEL_1', 'score': 0.9957847, 'index': 4, 'word': 'ĠSarah', 'start': 11, 'end': 16}, {'entity': 'LABEL_0', 'score': 0.9998487, 'index': 5, 'word': 'Ġand', 'start': 17, 'end': 20}, {'entity': 'LABEL_0', 'score': 0.9998658, 'index': 6, 'word': 'ĠI', 'start': 21, 'end': 22}, {'entity': 'LABEL_0', 'score': 0.99993086, 'index': 7, 'word': 'Ġlive', 'start': 23, 'end': 27}, {'entity': 'LABEL_0', 'score': 0.999918, 'index': 8, 'word': 'Ġin', 'start': 28, 'end': 30}, {'entity': 'LABEL_5', 'score': 0.99845695, 'index': 9, 'word': 'ĠLondon', 'start': 31, 'end': 37}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah and I live in London\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cc753d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.bart.configuration_bart.BartConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, LayoutLMv2Config, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, GPT2Config, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, LayoutLMConfig, Data2VecTextConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb#ch0000017?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb#ch0000017?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mslauw87/bart_summarisation\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb#ch0000017?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mslauw87/bart_summarisation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb#ch0000017?line=6'>7</a>\u001b[0m nlp \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m, model\u001b[39m=\u001b[39mmodel, tokenizer\u001b[39m=\u001b[39mtokenizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment5/serverless-bert/Untitled.ipynb#ch0000017?line=7'>8</a>\u001b[0m example \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMy name is Sarah and I live in London\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:447\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=444'>445</a>\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m    <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=445'>446</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=446'>447</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=447'>448</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=448'>449</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py?line=449'>450</a>\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.bart.configuration_bart.BartConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, LayoutLMv2Config, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, GPT2Config, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, LayoutLMConfig, Data2VecTextConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"slauw87/bart_summarisation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"slauw87/bart_summarisation\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah and I live in London\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5df7f3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 20920, 6, 127, 2335, 16, 11962, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74533a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "def get_model(model):\n",
    "  \"\"\"Loads model from Hugginface model hub\"\"\"\n",
    "  try:\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model)\n",
    "    model.save_pretrained('./model')\n",
    "  except Exception as e:\n",
    "    raise(e)\n",
    "\n",
    "def get_tokenizer(tokenizer):\n",
    "  \"\"\"Loads tokenizer from Hugginface model hub\"\"\"\n",
    "  try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    tokenizer.save_pretrained('./model')\n",
    "  except Exception as e:\n",
    "    raise(e)\n",
    "\n",
    "get_model('mrm8488/mobilebert-uncased-finetuned-squadv2')\n",
    "get_tokenizer('mrm8488/mobilebert-uncased-finetuned-squadv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a44b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version  \r\n",
      "------------------------ ---------\r\n",
      "absl-py                  0.15.0   \r\n",
      "aiohttp                  3.8.1    \r\n",
      "aiosignal                1.2.0    \r\n",
      "anyio                    3.5.0    \r\n",
      "appnope                  0.1.2    \r\n",
      "asgiref                  3.5.0    \r\n",
      "asttokens                2.0.5    \r\n",
      "astunparse               1.6.3    \r\n",
      "async-timeout            4.0.2    \r\n",
      "attrs                    21.4.0   \r\n",
      "backcall                 0.2.0    \r\n",
      "cachetools               5.0.0    \r\n",
      "certifi                  2021.10.8\r\n",
      "charset-normalizer       2.0.12   \r\n",
      "click                    8.0.4    \r\n",
      "cycler                   0.11.0   \r\n",
      "debugpy                  1.5.1    \r\n",
      "decorator                5.1.1    \r\n",
      "entrypoints              0.4      \r\n",
      "executing                0.8.3    \r\n",
      "fastapi                  0.75.0   \r\n",
      "filelock                 3.6.0    \r\n",
      "flatbuffers              1.12     \r\n",
      "fonttools                4.31.2   \r\n",
      "frozenlist               1.3.0    \r\n",
      "fsspec                   2022.2.0 \r\n",
      "gast                     0.3.3    \r\n",
      "gcsfs                    2022.2.0 \r\n",
      "geographiclib            1.52     \r\n",
      "geopy                    2.2.0    \r\n",
      "google-api-core          2.7.1    \r\n",
      "google-auth              2.6.2    \r\n",
      "google-auth-oauthlib     0.4.6    \r\n",
      "google-cloud-core        2.2.3    \r\n",
      "google-cloud-storage     2.2.1    \r\n",
      "google-crc32c            1.3.0    \r\n",
      "google-pasta             0.2.0    \r\n",
      "google-resumable-media   2.3.2    \r\n",
      "googleapis-common-protos 1.56.0   \r\n",
      "grpcio                   1.32.0   \r\n",
      "h11                      0.13.0   \r\n",
      "h5py                     2.10.0   \r\n",
      "huggingface-hub          0.5.1    \r\n",
      "idna                     3.3      \r\n",
      "importlib-metadata       4.11.3   \r\n",
      "ipykernel                6.9.2    \r\n",
      "ipython                  8.1.1    \r\n",
      "jedi                     0.18.1   \r\n",
      "joblib                   1.1.0    \r\n",
      "jupyter-client           7.1.2    \r\n",
      "jupyter-core             4.9.2    \r\n",
      "Keras-Preprocessing      1.1.2    \r\n",
      "kiwisolver               1.4.0    \r\n",
      "Markdown                 3.3.6    \r\n",
      "matplotlib               3.2.0    \r\n",
      "matplotlib-inline        0.1.3    \r\n",
      "multidict                6.0.2    \r\n",
      "nest-asyncio             1.5.4    \r\n",
      "numpy                    1.19.5   \r\n",
      "oauthlib                 3.2.0    \r\n",
      "opt-einsum               3.3.0    \r\n",
      "packaging                21.3     \r\n",
      "pandas                   1.4.1    \r\n",
      "parso                    0.8.3    \r\n",
      "pexpect                  4.8.0    \r\n",
      "pickleshare              0.7.5    \r\n",
      "Pillow                   9.0.1    \r\n",
      "pip                      19.2.3   \r\n",
      "prompt-toolkit           3.0.28   \r\n",
      "protobuf                 3.19.4   \r\n",
      "psutil                   5.9.0    \r\n",
      "ptyprocess               0.7.0    \r\n",
      "pure-eval                0.2.2    \r\n",
      "pyasn1                   0.4.8    \r\n",
      "pyasn1-modules           0.2.8    \r\n",
      "pydantic                 1.9.0    \r\n",
      "Pygments                 2.11.2   \r\n",
      "PyJWT                    2.3.0    \r\n",
      "pyparsing                3.0.7    \r\n",
      "python-dateutil          2.8.2    \r\n",
      "python-decouple          3.6      \r\n",
      "pytz                     2022.1   \r\n",
      "PyYAML                   6.0      \r\n",
      "pyzmq                    22.3.0   \r\n",
      "regex                    2022.3.15\r\n",
      "requests                 2.27.1   \r\n",
      "requests-oauthlib        1.3.1    \r\n",
      "rsa                      4.8      \r\n",
      "sacremoses               0.0.49   \r\n",
      "sentencepiece            0.1.96   \r\n",
      "setuptools               60.10.0  \r\n",
      "six                      1.16.0   \r\n",
      "sniffio                  1.2.0    \r\n",
      "stack-data               0.2.0    \r\n",
      "starlette                0.17.1   \r\n",
      "tensorboard              2.8.0    \r\n",
      "tensorboard-data-server  0.6.1    \r\n",
      "tensorboard-plugin-wit   1.8.1    \r\n",
      "tensorflow               2.4.1    \r\n",
      "tensorflow-estimator     2.4.0    \r\n",
      "termcolor                1.1.0    \r\n",
      "tokenizers               0.11.6   \r\n",
      "torch                    1.11.0   \r\n",
      "tornado                  6.1      \r\n",
      "tqdm                     4.64.0   \r\n",
      "traitlets                5.1.1    \r\n",
      "transformers             4.18.0   \r\n",
      "typing-extensions        3.7.4.3  \r\n",
      "urllib3                  1.26.9   \r\n",
      "uvicorn                  0.17.6   \r\n",
      "wcwidth                  0.2.5    \r\n",
      "Werkzeug                 2.0.3    \r\n",
      "wheel                    0.37.1   \r\n",
      "wrapt                    1.12.1   \r\n",
      "yarl                     1.7.2    \r\n",
      "zipp                     3.7.0    \r\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e41d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling transformers-3.4.0:\n",
      "  Would remove:\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/bin/transformers-cli\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers-3.4.0.dist-info/*\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall transformers == 4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651eb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting transformers==3.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: sacremoses in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (0.0.49)\n",
      "Requirement already satisfied: protobuf in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (3.19.4)\n",
      "Requirement already satisfied: packaging in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (21.3)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers==3.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/30/a47fb2ab4b1b214f3177efe700af7032b4ac229edae148a4f06016c8541a/sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl\n",
      "Requirement already satisfied: requests in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (2022.3.15)\n",
      "Requirement already satisfied: numpy in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (1.19.5)\n",
      "Collecting tokenizers==0.9.2 (from transformers==3.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/fe/cb38ae6a20143da7bf5427b1243fb6ccd238a0ca15393cf111ecc0176da8/tokenizers-0.9.2-cp38-cp38-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: joblib in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from packaging->transformers==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2021.10.8)\n",
      "Installing collected packages: sentencepiece, tokenizers, transformers\n",
      "  Found existing installation: tokenizers 0.11.6\n",
      "    Uninstalling tokenizers-0.11.6:\n",
      "      Successfully uninstalled tokenizers-0.11.6\n",
      "  Found existing installation: transformers 4.18.0\n",
      "    Uninstalling transformers-4.18.0:\n",
      "      Successfully uninstalled transformers-4.18.0\n",
      "Successfully installed sentencepiece-0.1.96 tokenizers-0.9.2 transformers-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48cb74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.18.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (0.5.1)\n",
      "Requirement already satisfied: requests in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/c2/34e886390a065b463bfa435426d990a50d2184fedab4e034a2dc8e37329d/tokenizers-0.11.6-cp38-cp38-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (1.19.5)\n",
      "Requirement already satisfied: filelock in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (0.0.49)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2.0.12)\n",
      "Requirement already satisfied: click in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (8.0.4)\n",
      "Requirement already satisfied: six in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.18.0) (3.0.7)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Found existing installation: tokenizers 0.9.2\n",
      "    Uninstalling tokenizers-0.9.2:\n",
      "      Successfully uninstalled tokenizers-0.9.2\n",
      "  Found existing installation: transformers 3.4.0\n",
      "    Uninstalling transformers-3.4.0:\n",
      "      Successfully uninstalled transformers-3.4.0\n",
      "Successfully installed tokenizers-0.11.6 transformers-4.18.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ccbb583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: 100%|██████████████████████████| 1.17k/1.17k [00:00<00:00, 325kB/s]\u001b[A\n",
      "\n",
      "Downloading:   0%|                                   | 0.00/850M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading:   0%|                          | 1.21M/850M [00:00<01:10, 12.7MB/s]\u001b[A\n",
      "Downloading:   1%|▏                         | 6.48M/850M [00:00<00:23, 37.7MB/s]\u001b[A\n",
      "Downloading:   1%|▎                         | 10.3M/850M [00:00<00:23, 37.6MB/s]\u001b[A\n",
      "Downloading:   2%|▌                         | 17.3M/850M [00:00<00:17, 51.3MB/s]\u001b[A\n",
      "Downloading:   3%|▋                         | 23.1M/850M [00:00<00:15, 55.0MB/s]\u001b[A\n",
      "Downloading:   3%|▊                         | 28.4M/850M [00:00<00:20, 41.9MB/s]\u001b[A\n",
      "Downloading:   4%|█                         | 32.8M/850M [00:00<00:21, 40.2MB/s]\u001b[A\n",
      "Downloading:   4%|█▏                        | 36.9M/850M [00:00<00:21, 40.5MB/s]\u001b[A\n",
      "Downloading:   5%|█▎                        | 44.6M/850M [00:01<00:16, 51.4MB/s]\u001b[A\n",
      "Downloading:   6%|█▌                        | 50.3M/850M [00:01<00:15, 53.3MB/s]\u001b[A\n",
      "Downloading:   7%|█▋                        | 56.0M/850M [00:01<00:15, 55.2MB/s]\u001b[A\n",
      "Downloading:   7%|█▉                        | 62.2M/850M [00:01<00:14, 58.0MB/s]\u001b[A\n",
      "Downloading:   8%|██                        | 69.1M/850M [00:01<00:13, 62.4MB/s]\u001b[A\n",
      "Downloading:   9%|██▎                       | 75.2M/850M [00:01<00:13, 60.5MB/s]\u001b[A\n",
      "Downloading:  10%|██▍                       | 81.1M/850M [00:01<00:14, 55.3MB/s]\u001b[A\n",
      "Downloading:  10%|██▋                       | 88.0M/850M [00:01<00:14, 53.6MB/s]\u001b[A\n",
      "Downloading:  11%|██▉                       | 94.2M/850M [00:01<00:14, 56.5MB/s]\u001b[A\n",
      "Downloading:  12%|███                       | 99.7M/850M [00:02<00:14, 55.4MB/s]\u001b[A\n",
      "Downloading:  12%|███▎                       | 105M/850M [00:02<00:14, 54.9MB/s]\u001b[A\n",
      "Downloading:  13%|███▌                       | 111M/850M [00:02<00:13, 58.1MB/s]\u001b[A\n",
      "Downloading:  14%|███▋                       | 117M/850M [00:02<00:14, 53.7MB/s]\u001b[A\n",
      "Downloading:  14%|███▉                       | 122M/850M [00:02<00:17, 44.7MB/s]\u001b[A\n",
      "Downloading:  15%|████                       | 128M/850M [00:02<00:15, 48.6MB/s]\u001b[A\n",
      "Downloading:  16%|████▏                      | 133M/850M [00:02<00:16, 46.9MB/s]\u001b[A\n",
      "Downloading:  16%|████▎                      | 138M/850M [00:02<00:15, 47.5MB/s]\u001b[A\n",
      "Downloading:  17%|████▌                      | 143M/850M [00:02<00:14, 49.9MB/s]\u001b[A\n",
      "Downloading:  18%|████▋                      | 149M/850M [00:03<00:13, 53.3MB/s]\u001b[A\n",
      "Downloading:  18%|████▉                      | 155M/850M [00:03<00:12, 57.0MB/s]\u001b[A\n",
      "Downloading:  19%|█████                      | 161M/850M [00:03<00:14, 49.4MB/s]\u001b[A\n",
      "Downloading:  19%|█████▎                     | 166M/850M [00:03<00:14, 49.5MB/s]\u001b[A\n",
      "Downloading:  20%|█████▍                     | 171M/850M [00:03<00:15, 45.2MB/s]\u001b[A\n",
      "Downloading:  21%|█████▌                     | 175M/850M [00:03<00:15, 44.5MB/s]\u001b[A\n",
      "Downloading:  21%|█████▋                     | 181M/850M [00:03<00:15, 44.9MB/s]\u001b[A\n",
      "Downloading:  22%|█████▉                     | 185M/850M [00:03<00:18, 38.1MB/s]\u001b[A\n",
      "Downloading:  22%|██████                     | 189M/850M [00:04<00:18, 37.4MB/s]\u001b[A\n",
      "Downloading:  23%|██████                     | 193M/850M [00:04<00:18, 37.6MB/s]\u001b[A\n",
      "Downloading:  23%|██████▎                    | 199M/850M [00:04<00:14, 45.9MB/s]\u001b[A\n",
      "Downloading:  24%|██████▍                    | 204M/850M [00:04<00:19, 34.5MB/s]\u001b[A\n",
      "Downloading:  24%|██████▌                    | 208M/850M [00:04<00:20, 33.1MB/s]\u001b[A\n",
      "Downloading:  25%|██████▋                    | 211M/850M [00:04<00:21, 31.5MB/s]\u001b[A\n",
      "Downloading:  25%|██████▊                    | 215M/850M [00:04<00:19, 34.2MB/s]\u001b[A\n",
      "Downloading:  26%|██████▉                    | 219M/850M [00:05<00:24, 27.1MB/s]\u001b[A\n",
      "Downloading:  26%|███████                    | 224M/850M [00:05<00:20, 32.8MB/s]\u001b[A\n",
      "Downloading:  27%|███████▎                   | 230M/850M [00:05<00:16, 40.5MB/s]\u001b[A\n",
      "Downloading:  28%|███████▍                   | 235M/850M [00:05<00:16, 40.2MB/s]\u001b[A\n",
      "Downloading:  28%|███████▌                   | 240M/850M [00:05<00:16, 38.2MB/s]\u001b[A\n",
      "Downloading:  29%|███████▊                   | 246M/850M [00:05<00:15, 42.0MB/s]\u001b[A\n",
      "Downloading:  30%|████████                   | 254M/850M [00:05<00:12, 51.4MB/s]\u001b[A\n",
      "Downloading:  30%|████████▏                  | 259M/850M [00:05<00:12, 48.8MB/s]\u001b[A\n",
      "Downloading:  31%|████████▍                  | 264M/850M [00:06<00:16, 38.3MB/s]\u001b[A\n",
      "Downloading:  32%|████████▌                  | 269M/850M [00:06<00:14, 40.8MB/s]\u001b[A\n",
      "Downloading:  32%|████████▋                  | 274M/850M [00:06<00:13, 44.9MB/s]\u001b[A\n",
      "Downloading:  33%|████████▉                  | 281M/850M [00:06<00:11, 51.0MB/s]\u001b[A\n",
      "Downloading:  34%|█████████                  | 286M/850M [00:06<00:11, 53.1MB/s]\u001b[A\n",
      "Downloading:  34%|█████████▎                 | 292M/850M [00:06<00:11, 49.0MB/s]\u001b[A\n",
      "Downloading:  35%|█████████▍                 | 297M/850M [00:06<00:13, 42.1MB/s]\u001b[A\n",
      "Downloading:  36%|█████████▋                 | 303M/850M [00:06<00:11, 49.0MB/s]\u001b[A\n",
      "Downloading:  36%|█████████▊                 | 310M/850M [00:07<00:10, 53.7MB/s]\u001b[A\n",
      "Downloading:  37%|██████████                 | 316M/850M [00:07<00:09, 57.1MB/s]\u001b[A\n",
      "Downloading:  38%|██████████▏                | 322M/850M [00:07<00:09, 55.5MB/s]\u001b[A\n",
      "Downloading:  39%|██████████▍                | 328M/850M [00:07<00:09, 57.4MB/s]\u001b[A\n",
      "Downloading:  39%|██████████▌                | 334M/850M [00:07<00:09, 59.0MB/s]\u001b[A\n",
      "Downloading:  40%|██████████▊                | 340M/850M [00:07<00:08, 61.4MB/s]\u001b[A\n",
      "Downloading:  41%|██████████▉                | 346M/850M [00:07<00:10, 48.9MB/s]\u001b[A\n",
      "Downloading:  41%|███████████▏               | 352M/850M [00:07<00:11, 44.3MB/s]\u001b[A\n",
      "Downloading:  42%|███████████▎               | 358M/850M [00:08<00:10, 47.5MB/s]\u001b[A\n",
      "Downloading:  43%|███████████▌               | 363M/850M [00:08<00:10, 49.7MB/s]\u001b[A\n",
      "Downloading:  43%|███████████▋               | 368M/850M [00:08<00:09, 50.8MB/s]\u001b[A\n",
      "Downloading:  44%|███████████▊               | 373M/850M [00:08<00:10, 49.4MB/s]\u001b[A\n",
      "Downloading:  44%|████████████               | 378M/850M [00:08<00:10, 48.5MB/s]\u001b[A\n",
      "Downloading:  45%|████████████▏              | 385M/850M [00:08<00:09, 53.1MB/s]\u001b[A\n",
      "Downloading:  46%|████████████▎              | 390M/850M [00:08<00:09, 52.5MB/s]\u001b[A\n",
      "Downloading:  47%|████████████▌              | 396M/850M [00:08<00:08, 57.2MB/s]\u001b[A\n",
      "Downloading:  47%|████████████▊              | 402M/850M [00:08<00:10, 46.6MB/s]\u001b[A\n",
      "Downloading:  48%|████████████▉              | 407M/850M [00:09<00:09, 48.8MB/s]\u001b[A\n",
      "Downloading:  21%|█████▎                    | 448M/2.12G [07:15<27:54, 1.08MB/s]\u001b[A\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load the model for 't5-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 't5-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:1671\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m-> 1671\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m        \u001b[49m\u001b[43marchive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/utils/hub.py:282\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/utils/hub.py:585\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    583\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 585\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/utils/hub.py:440\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m    439\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[0;32m--> 440\u001b[0m         \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m progress\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/tempfile.py:474\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;129m@_functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5ForConditionalGeneration, T5Tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# initialize the model architecture and weights\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt5-base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# initialize the model tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:1761\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1753\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this model, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the cached \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1759\u001b[0m     )\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m-> 1761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1762\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the model for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1764\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1765\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1766\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1767\u001b[0m     )\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;241m==\u001b[39m archive_file:\n\u001b[1;32m   1770\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading weights file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the model for 't5-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 't5-base' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading:  49%|█████████████              | 412M/850M [00:29<00:15, 29.9MB/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# initialize the model architecture and weights\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "# initialize the model tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed5ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4268bdeb8b3cd6119633cfedec5f8ccd1d344ae93aca82ac0bac4da2ecd2271"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': venv)",
   "language": "python",
   "name": "python380jvsc74a57bd0b4268bdeb8b3cd6119633cfedec5f8ccd1d344ae93aca82ac0bac4da2ecd2271"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
