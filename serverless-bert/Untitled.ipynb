{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b66c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17226ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = json.loads(event['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "240833e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the GLUE score for Bert?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body['context']\n",
    "body['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "424d7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\"body\":\"{\\\"context\\\":\\\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\\\",\\n\\\"question\\\":\\\"What is the GLUE score for Bert?\\\"\\n}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96428d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adba627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"context\":\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\",\\n\"question\":\"What is the GLUE score for Bert?\"\\n}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335f795d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "event['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b968bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoConfig\n",
    "\n",
    "def encode(tokenizer, question, context):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(question, context)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "def decode(tokenizer, token):\n",
    "    \"\"\"decodes the tokens to the answer with a given tokenizer\"\"\"\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        token, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "def serverless_pipeline(model_path='./model'):\n",
    "    \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "    def predict(question, context):\n",
    "        \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\"\n",
    "        input_ids, attention_mask = encode(tokenizer,question, context)\n",
    "        a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "        start_scores = a['start_logits']\n",
    "        end_scores = a['end_logits']\n",
    "        ans_tokens = input_ids[torch.argmax(\n",
    "            start_scores): torch.argmax(end_scores)+1]\n",
    "        answer = decode(tokenizer,ans_tokens)\n",
    "        return answer\n",
    "    return predict\n",
    "\n",
    "# initializes the pipeline\n",
    "question_answering_pipeline = serverless_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2e07325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80. 5 %'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path='./model'\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "input_ids, attention_mask = encode(tokenizer,body['question'], body['context'])\n",
    "a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "start_scores = a['start_logits']\n",
    "end_scores = a['end_logits']\n",
    "ans_tokens = input_ids[torch.argmax(\n",
    "            start_scores): torch.argmax(end_scores)+1]\n",
    "answer = decode(tokenizer,ans_tokens)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5cae9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(180)\n",
      "tensor(184)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(start_scores))\n",
    "print(torch.argmax(end_scores)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc8a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = question_answering_pipeline(question=body['question'], context=body['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a3a83513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.9935,  -9.6032, -12.6266,  -8.8786,  -9.7391, -10.4422, -13.1308,\n",
       "         -10.5575, -13.7492, -11.6378,  -8.5799, -10.5312,  -9.8652,  -9.7580,\n",
       "          -9.4949, -11.9562, -12.3860, -12.0936,  -8.6679, -12.0692, -10.2790,\n",
       "         -10.8464, -12.4050,  -7.5609, -12.8723, -12.9467, -13.6367,  -9.9548,\n",
       "         -12.5127, -13.8898, -10.5344, -12.8058,  -9.0189, -11.6492, -10.1382,\n",
       "          -9.3935,  -9.7570, -12.3048, -12.3742, -10.0070,  -7.4577, -11.7353,\n",
       "         -13.9000, -14.5311, -12.7638,  -8.2836, -13.4187, -11.9397,  -7.5125,\n",
       "         -12.0522, -11.9577, -13.0190, -13.8692, -13.9177, -12.6028,  -8.6266,\n",
       "         -12.6308, -11.2395,  -8.2875, -11.5319,  -9.7516,  -9.8484,  -8.8828,\n",
       "         -12.1323,  -9.1497,  -9.5073, -14.3122, -14.7056, -15.2779, -12.3574,\n",
       "         -12.2021,  -8.8719, -12.2928, -13.3568, -13.6476, -10.2934,  -9.7695,\n",
       "          -8.4249,  -9.5299, -10.4064,  -8.4501,  -8.1455, -13.3144,  -9.5901,\n",
       "          -9.9255,  -9.9359,  -8.9109, -10.1722, -10.9245, -10.2094, -11.4849,\n",
       "         -12.8439, -11.3473,  -7.2822,  -7.7438, -12.2276, -10.2857,  -9.2049,\n",
       "         -11.3570, -10.1022, -12.0572,  -8.6917, -12.5233, -13.2024, -10.5926,\n",
       "          -8.1687,  -6.5877,  -9.6355,  -9.4750, -10.5913, -10.6655, -10.7288,\n",
       "          -9.7544, -14.1949, -12.8981, -13.8851, -11.2254, -14.0085, -12.2946,\n",
       "         -11.9040, -12.2679,  -9.8179,  -9.8613, -12.0148, -15.1357, -12.1575,\n",
       "         -11.7155, -11.8134, -13.9296,  -9.4054, -12.0592, -13.9847,  -9.5476,\n",
       "         -12.0704, -13.0676,  -9.9968,  -8.8959,  -9.2934, -11.9428, -13.6875,\n",
       "         -14.3590, -14.0119, -10.9463, -10.7517, -10.8265,  -7.8211, -11.1233,\n",
       "          -8.5499, -13.8656,  -8.8626, -13.8940,  -9.5735, -14.5186, -10.1458,\n",
       "         -10.6286,  -6.7452,  -9.0446, -13.2036,  -8.1718,  -8.4822, -13.1104,\n",
       "         -11.9360, -12.8214, -10.6498, -13.0211, -10.9861,  -9.8080,  -8.9694,\n",
       "          -6.9046,  -8.5696,  -8.3411, -10.3213, -10.1375,  -9.6618,  -7.4663,\n",
       "          -4.4304,  -4.1391,  -5.1907,  -7.4831,  -3.0677,   2.3072,  -7.7782,\n",
       "          -6.5240,  -4.7309,  -4.8970,  -0.3418,  -9.9202,  -7.7213,  -8.0191,\n",
       "          -8.0278,  -5.9619,  -8.0912,  -8.5301,  -9.8071,  -4.8425,  -9.9268,\n",
       "         -10.6415,  -7.5797,  -7.0113,  -2.6098, -11.2151,  -9.6228,  -8.0463,\n",
       "          -7.6116,  -4.1799, -11.3936,  -9.0563,  -9.1889,  -8.6666, -10.1769,\n",
       "         -10.1953, -11.2783,  -6.2193,  -8.7358,  -9.7729, -12.4952,  -9.9106,\n",
       "          -8.8787, -10.7488, -10.8436,  -8.5129,  -9.9556,  -5.1896, -12.7473,\n",
       "          -9.7157, -10.8408,  -5.6944, -11.8643,  -9.6395, -12.0872, -10.6213,\n",
       "         -12.0420, -12.0264, -11.5038,  -6.4843,  -8.9994, -10.1185, -13.1069,\n",
       "         -10.8206, -10.1366,  -9.0342,  -9.5118,  -4.5315, -12.5122,  -9.6609,\n",
       "         -10.1086,  -5.9065, -12.2503,  -9.9088, -11.9210, -10.8050, -11.9623,\n",
       "         -11.8584,  -9.5306,  -9.3333]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "724ba745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2054,\n",
       " 2003,\n",
       " 1996,\n",
       " 25238,\n",
       " 3556,\n",
       " 2005,\n",
       " 14324,\n",
       " 1029,\n",
       " 102,\n",
       " 2057,\n",
       " 8970,\n",
       " 1037,\n",
       " 2047,\n",
       " 2653,\n",
       " 6630,\n",
       " 2944,\n",
       " 2170,\n",
       " 14324,\n",
       " 1010,\n",
       " 2029,\n",
       " 4832,\n",
       " 2005,\n",
       " 7226,\n",
       " 7442,\n",
       " 7542,\n",
       " 2389,\n",
       " 4372,\n",
       " 16044,\n",
       " 2099,\n",
       " 15066,\n",
       " 2013,\n",
       " 19081,\n",
       " 1012,\n",
       " 4406,\n",
       " 3522,\n",
       " 2653,\n",
       " 6630,\n",
       " 4275,\n",
       " 1006,\n",
       " 12420,\n",
       " 3802,\n",
       " 2632,\n",
       " 1012,\n",
       " 1010,\n",
       " 2760,\n",
       " 2050,\n",
       " 1025,\n",
       " 10958,\n",
       " 20952,\n",
       " 8551,\n",
       " 3802,\n",
       " 2632,\n",
       " 1012,\n",
       " 1010,\n",
       " 2760,\n",
       " 1007,\n",
       " 1010,\n",
       " 14324,\n",
       " 2003,\n",
       " 2881,\n",
       " 2000,\n",
       " 3653,\n",
       " 23654,\n",
       " 2784,\n",
       " 7226,\n",
       " 7442,\n",
       " 7542,\n",
       " 2389,\n",
       " 15066,\n",
       " 2013,\n",
       " 4895,\n",
       " 20470,\n",
       " 12260,\n",
       " 2094,\n",
       " 3793,\n",
       " 2011,\n",
       " 10776,\n",
       " 14372,\n",
       " 2006,\n",
       " 2119,\n",
       " 2187,\n",
       " 1998,\n",
       " 2157,\n",
       " 6123,\n",
       " 1999,\n",
       " 2035,\n",
       " 9014,\n",
       " 1012,\n",
       " 2004,\n",
       " 1037,\n",
       " 2765,\n",
       " 1010,\n",
       " 1996,\n",
       " 3653,\n",
       " 1011,\n",
       " 4738,\n",
       " 14324,\n",
       " 2944,\n",
       " 2064,\n",
       " 2022,\n",
       " 2986,\n",
       " 8525,\n",
       " 7228,\n",
       " 2007,\n",
       " 2074,\n",
       " 2028,\n",
       " 3176,\n",
       " 6434,\n",
       " 6741,\n",
       " 2000,\n",
       " 3443,\n",
       " 2110,\n",
       " 1011,\n",
       " 1997,\n",
       " 1011,\n",
       " 1996,\n",
       " 1011,\n",
       " 2396,\n",
       " 4275,\n",
       " 2005,\n",
       " 1037,\n",
       " 2898,\n",
       " 2846,\n",
       " 1997,\n",
       " 8518,\n",
       " 1010,\n",
       " 2107,\n",
       " 2004,\n",
       " 3160,\n",
       " 10739,\n",
       " 1998,\n",
       " 2653,\n",
       " 28937,\n",
       " 1010,\n",
       " 2302,\n",
       " 6937,\n",
       " 8518,\n",
       " 5051,\n",
       " 6895,\n",
       " 8873,\n",
       " 2278,\n",
       " 4294,\n",
       " 12719,\n",
       " 1012,\n",
       " 14324,\n",
       " 2003,\n",
       " 17158,\n",
       " 2135,\n",
       " 3722,\n",
       " 1998,\n",
       " 17537,\n",
       " 2135,\n",
       " 3928,\n",
       " 1012,\n",
       " 2009,\n",
       " 6855,\n",
       " 2015,\n",
       " 2047,\n",
       " 2110,\n",
       " 1011,\n",
       " 1997,\n",
       " 1011,\n",
       " 1996,\n",
       " 1011,\n",
       " 2396,\n",
       " 3463,\n",
       " 2006,\n",
       " 5408,\n",
       " 3019,\n",
       " 2653,\n",
       " 6364,\n",
       " 8518,\n",
       " 1010,\n",
       " 2164,\n",
       " 6183,\n",
       " 1996,\n",
       " 25238,\n",
       " 3556,\n",
       " 2000,\n",
       " 3770,\n",
       " 1012,\n",
       " 1019,\n",
       " 1003,\n",
       " 1006,\n",
       " 1021,\n",
       " 1012,\n",
       " 1021,\n",
       " 1003,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1010,\n",
       " 4800,\n",
       " 20554,\n",
       " 2072,\n",
       " 10640,\n",
       " 2000,\n",
       " 6564,\n",
       " 1012,\n",
       " 1021,\n",
       " 1003,\n",
       " 1006,\n",
       " 1018,\n",
       " 1012,\n",
       " 1020,\n",
       " 1003,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1010,\n",
       " 4686,\n",
       " 1058,\n",
       " 2487,\n",
       " 1012,\n",
       " 1015,\n",
       " 3160,\n",
       " 10739,\n",
       " 3231,\n",
       " 20069,\n",
       " 2000,\n",
       " 6109,\n",
       " 1012,\n",
       " 1016,\n",
       " 1006,\n",
       " 1015,\n",
       " 1012,\n",
       " 1019,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1998,\n",
       " 4686,\n",
       " 1058,\n",
       " 2475,\n",
       " 1012,\n",
       " 1014,\n",
       " 3231,\n",
       " 20069,\n",
       " 2000,\n",
       " 6640,\n",
       " 1012,\n",
       " 1015,\n",
       " 1006,\n",
       " 1019,\n",
       " 1012,\n",
       " 1015,\n",
       " 2391,\n",
       " 7619,\n",
       " 7620,\n",
       " 1007,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdeb5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb7b69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='hoanhkhoa/roberta-base-finetuned-ner', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6dfa229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 20920, 6, 127, 2335, 16, 11962, 2] [1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def encode(tokenizer, question):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(question)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "def decode(tokenizer, token):\n",
    "    \"\"\"decodes the tokens to the answer with a given tokenizer\"\"\"\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(\n",
    "        token, skip_special_tokens=True)\n",
    "    return tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "\n",
    "input_ids, attention_mask = encode(tokenizer,\"Hello, my dog is cute\")\n",
    "print(input_ids, attention_mask)\n",
    "a = model(torch.tensor(\n",
    "            [input_ids]), attention_mask=torch.tensor([attention_mask]))\n",
    "ans_tokens= input_ids[4]\n",
    "answer = decode(tokenizer,ans_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc168a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.9998253, 'index': 1, 'word': 'ĠMy', 'start': 0, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.9998684, 'index': 2, 'word': 'Ġname', 'start': 3, 'end': 7}, {'entity': 'LABEL_0', 'score': 0.99989426, 'index': 3, 'word': 'Ġis', 'start': 8, 'end': 10}, {'entity': 'LABEL_1', 'score': 0.9957847, 'index': 4, 'word': 'ĠSarah', 'start': 11, 'end': 16}, {'entity': 'LABEL_0', 'score': 0.9998487, 'index': 5, 'word': 'Ġand', 'start': 17, 'end': 20}, {'entity': 'LABEL_0', 'score': 0.9998658, 'index': 6, 'word': 'ĠI', 'start': 21, 'end': 22}, {'entity': 'LABEL_0', 'score': 0.99993086, 'index': 7, 'word': 'Ġlive', 'start': 23, 'end': 27}, {'entity': 'LABEL_0', 'score': 0.999918, 'index': 8, 'word': 'Ġin', 'start': 28, 'end': 30}, {'entity': 'LABEL_5', 'score': 0.99845695, 'index': 9, 'word': 'ĠLondon', 'start': 31, 'end': 37}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"hoanhkhoa/roberta-base-finetuned-ner\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah and I live in London\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4cc753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████| 300/300 [00:00<00:00, 63.1kB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.59k/1.59k [00:00<00:00, 311kB/s]\n",
      "Downloading: 100%|███████████████████████████| 780k/780k [00:00<00:00, 4.99MB/s]\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 3.52MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.29M/1.29M [00:00<00:00, 7.44MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 239/239 [00:00<00:00, 47.7kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.bart.configuration_bart.BartConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, LayoutLMv2Config, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, GPT2Config, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, LayoutLMConfig, Data2VecTextConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslauw87/bart_summarisation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslauw87/bart_summarisation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      8\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy name is Sarah and I live in London\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:447\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    450\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.bart.configuration_bart.BartConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\nModel type should be one of YosoConfig, NystromformerConfig, QDQBertConfig, FNetConfig, LayoutLMv2Config, RemBertConfig, CanineConfig, RoFormerConfig, BigBirdConfig, ConvBertConfig, IBertConfig, MobileBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaXLConfig, XLMRobertaConfig, MegatronBertConfig, MPNetConfig, LongformerConfig, RobertaConfig, DebertaV2Config, DebertaConfig, FlaubertConfig, SqueezeBertConfig, BertConfig, GPT2Config, XLNetConfig, XLMConfig, ElectraConfig, FunnelConfig, LayoutLMConfig, Data2VecTextConfig."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"slauw87/bart_summarisation\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"slauw87/bart_summarisation\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Sarah and I live in London\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5df7f3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 20920, 6, 127, 2335, 16, 11962, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74533a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "def get_model(model):\n",
    "  \"\"\"Loads model from Hugginface model hub\"\"\"\n",
    "  try:\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model)\n",
    "    model.save_pretrained('./model')\n",
    "  except Exception as e:\n",
    "    raise(e)\n",
    "\n",
    "def get_tokenizer(tokenizer):\n",
    "  \"\"\"Loads tokenizer from Hugginface model hub\"\"\"\n",
    "  try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "    tokenizer.save_pretrained('./model')\n",
    "  except Exception as e:\n",
    "    raise(e)\n",
    "\n",
    "get_model('mrm8488/mobilebert-uncased-finetuned-squadv2')\n",
    "get_tokenizer('mrm8488/mobilebert-uncased-finetuned-squadv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a44b8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version  \r\n",
      "------------------------ ---------\r\n",
      "absl-py                  0.15.0   \r\n",
      "aiohttp                  3.8.1    \r\n",
      "aiosignal                1.2.0    \r\n",
      "anyio                    3.5.0    \r\n",
      "appnope                  0.1.2    \r\n",
      "asgiref                  3.5.0    \r\n",
      "asttokens                2.0.5    \r\n",
      "astunparse               1.6.3    \r\n",
      "async-timeout            4.0.2    \r\n",
      "attrs                    21.4.0   \r\n",
      "backcall                 0.2.0    \r\n",
      "cachetools               5.0.0    \r\n",
      "certifi                  2021.10.8\r\n",
      "charset-normalizer       2.0.12   \r\n",
      "click                    8.0.4    \r\n",
      "cycler                   0.11.0   \r\n",
      "debugpy                  1.5.1    \r\n",
      "decorator                5.1.1    \r\n",
      "entrypoints              0.4      \r\n",
      "executing                0.8.3    \r\n",
      "fastapi                  0.75.0   \r\n",
      "filelock                 3.6.0    \r\n",
      "flatbuffers              1.12     \r\n",
      "fonttools                4.31.2   \r\n",
      "frozenlist               1.3.0    \r\n",
      "fsspec                   2022.2.0 \r\n",
      "gast                     0.3.3    \r\n",
      "gcsfs                    2022.2.0 \r\n",
      "geographiclib            1.52     \r\n",
      "geopy                    2.2.0    \r\n",
      "google-api-core          2.7.1    \r\n",
      "google-auth              2.6.2    \r\n",
      "google-auth-oauthlib     0.4.6    \r\n",
      "google-cloud-core        2.2.3    \r\n",
      "google-cloud-storage     2.2.1    \r\n",
      "google-crc32c            1.3.0    \r\n",
      "google-pasta             0.2.0    \r\n",
      "google-resumable-media   2.3.2    \r\n",
      "googleapis-common-protos 1.56.0   \r\n",
      "grpcio                   1.32.0   \r\n",
      "h11                      0.13.0   \r\n",
      "h5py                     2.10.0   \r\n",
      "huggingface-hub          0.5.1    \r\n",
      "idna                     3.3      \r\n",
      "importlib-metadata       4.11.3   \r\n",
      "ipykernel                6.9.2    \r\n",
      "ipython                  8.1.1    \r\n",
      "jedi                     0.18.1   \r\n",
      "joblib                   1.1.0    \r\n",
      "jupyter-client           7.1.2    \r\n",
      "jupyter-core             4.9.2    \r\n",
      "Keras-Preprocessing      1.1.2    \r\n",
      "kiwisolver               1.4.0    \r\n",
      "Markdown                 3.3.6    \r\n",
      "matplotlib               3.2.0    \r\n",
      "matplotlib-inline        0.1.3    \r\n",
      "multidict                6.0.2    \r\n",
      "nest-asyncio             1.5.4    \r\n",
      "numpy                    1.19.5   \r\n",
      "oauthlib                 3.2.0    \r\n",
      "opt-einsum               3.3.0    \r\n",
      "packaging                21.3     \r\n",
      "pandas                   1.4.1    \r\n",
      "parso                    0.8.3    \r\n",
      "pexpect                  4.8.0    \r\n",
      "pickleshare              0.7.5    \r\n",
      "Pillow                   9.0.1    \r\n",
      "pip                      19.2.3   \r\n",
      "prompt-toolkit           3.0.28   \r\n",
      "protobuf                 3.19.4   \r\n",
      "psutil                   5.9.0    \r\n",
      "ptyprocess               0.7.0    \r\n",
      "pure-eval                0.2.2    \r\n",
      "pyasn1                   0.4.8    \r\n",
      "pyasn1-modules           0.2.8    \r\n",
      "pydantic                 1.9.0    \r\n",
      "Pygments                 2.11.2   \r\n",
      "PyJWT                    2.3.0    \r\n",
      "pyparsing                3.0.7    \r\n",
      "python-dateutil          2.8.2    \r\n",
      "python-decouple          3.6      \r\n",
      "pytz                     2022.1   \r\n",
      "PyYAML                   6.0      \r\n",
      "pyzmq                    22.3.0   \r\n",
      "regex                    2022.3.15\r\n",
      "requests                 2.27.1   \r\n",
      "requests-oauthlib        1.3.1    \r\n",
      "rsa                      4.8      \r\n",
      "sacremoses               0.0.49   \r\n",
      "sentencepiece            0.1.96   \r\n",
      "setuptools               60.10.0  \r\n",
      "six                      1.16.0   \r\n",
      "sniffio                  1.2.0    \r\n",
      "stack-data               0.2.0    \r\n",
      "starlette                0.17.1   \r\n",
      "tensorboard              2.8.0    \r\n",
      "tensorboard-data-server  0.6.1    \r\n",
      "tensorboard-plugin-wit   1.8.1    \r\n",
      "tensorflow               2.4.1    \r\n",
      "tensorflow-estimator     2.4.0    \r\n",
      "termcolor                1.1.0    \r\n",
      "tokenizers               0.11.6   \r\n",
      "torch                    1.11.0   \r\n",
      "tornado                  6.1      \r\n",
      "tqdm                     4.64.0   \r\n",
      "traitlets                5.1.1    \r\n",
      "transformers             4.18.0   \r\n",
      "typing-extensions        3.7.4.3  \r\n",
      "urllib3                  1.26.9   \r\n",
      "uvicorn                  0.17.6   \r\n",
      "wcwidth                  0.2.5    \r\n",
      "Werkzeug                 2.0.3    \r\n",
      "wheel                    0.37.1   \r\n",
      "wrapt                    1.12.1   \r\n",
      "yarl                     1.7.2    \r\n",
      "zipp                     3.7.0    \r\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76e41d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling transformers-3.4.0:\n",
      "  Would remove:\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/bin/transformers-cli\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers-3.4.0.dist-info/*\n",
      "    /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages/transformers/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall transformers == 4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651eb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting transformers==3.4.0\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: sacremoses in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (0.0.49)\n",
      "Requirement already satisfied: protobuf in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (3.19.4)\n",
      "Requirement already satisfied: packaging in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (21.3)\n",
      "Collecting sentencepiece!=0.1.92 (from transformers==3.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f8/30/a47fb2ab4b1b214f3177efe700af7032b4ac229edae148a4f06016c8541a/sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl\n",
      "Requirement already satisfied: requests in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (4.64.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (2022.3.15)\n",
      "Requirement already satisfied: numpy in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==3.4.0) (1.19.5)\n",
      "Collecting tokenizers==0.9.2 (from transformers==3.4.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/f3/fe/cb38ae6a20143da7bf5427b1243fb6ccd238a0ca15393cf111ecc0176da8/tokenizers-0.9.2-cp38-cp38-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: joblib in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==3.4.0) (8.0.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from packaging->transformers==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==3.4.0) (2021.10.8)\n",
      "Installing collected packages: sentencepiece, tokenizers, transformers\n",
      "  Found existing installation: tokenizers 0.11.6\n",
      "    Uninstalling tokenizers-0.11.6:\n",
      "      Successfully uninstalled tokenizers-0.11.6\n",
      "  Found existing installation: transformers 4.18.0\n",
      "    Uninstalling transformers-4.18.0:\n",
      "      Successfully uninstalled transformers-4.18.0\n",
      "Successfully installed sentencepiece-0.1.96 tokenizers-0.9.2 transformers-3.4.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48cb74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.18.0\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (0.5.1)\n",
      "Requirement already satisfied: requests in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/ab/c2/34e886390a065b463bfa435426d990a50d2184fedab4e034a2dc8e37329d/tokenizers-0.11.6-cp38-cp38-macosx_10_11_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (1.19.5)\n",
      "Requirement already satisfied: filelock in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (0.0.49)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from transformers==4.18.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from requests->transformers==4.18.0) (2.0.12)\n",
      "Requirement already satisfied: click in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (8.0.4)\n",
      "Requirement already satisfied: six in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from sacremoses->transformers==4.18.0) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/parthshah/Documents/Northeastern/Spring2022/BigDataAnalytics/Assignment3/.venv/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.18.0) (3.0.7)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Found existing installation: tokenizers 0.9.2\n",
      "    Uninstalling tokenizers-0.9.2:\n",
      "      Successfully uninstalled tokenizers-0.9.2\n",
      "  Found existing installation: transformers 3.4.0\n",
      "    Uninstalling transformers-3.4.0:\n",
      "      Successfully uninstalled transformers-3.4.0\n",
      "Successfully installed tokenizers-0.11.6 transformers-4.18.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbb583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': venv)",
   "language": "python",
   "name": "python380jvsc74a57bd0b4268bdeb8b3cd6119633cfedec5f8ccd1d344ae93aca82ac0bac4da2ecd2271"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
